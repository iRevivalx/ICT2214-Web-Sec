{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train JSONL created: 'gpt4o_php_python_train.jsonl' (592 samples)\n",
      " Validation JSONL created: 'gpt4o_php_python_val.jsonl' (255 samples)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load PHP & Python dataset\n",
    "df = pd.concat([\n",
    "    pd.read_csv(\"python_vuln_CyberNative.csv\", encoding=\"utf-8\"),\n",
    "    pd.read_csv(\"php_vuln_CyberNative.csv\", encoding=\"utf-8\")\n",
    "])\n",
    "\n",
    "# Fill missing values\n",
    "df = df.fillna(\"Unknown\")\n",
    "\n",
    "# Rename columns for clarity\n",
    "df.rename(columns={\"lang\": \"language\", \"chosen\": \"secure_code\", \"rejected\": \"insecure_code\"}, inplace=True)\n",
    "\n",
    "# Split into 70% Train & 30% Validation\n",
    "train_df, val_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "# Function to Save Data as JSONL\n",
    "def save_to_jsonl(dataframe, filename):\n",
    "    \"\"\"Convert dataset to OpenAI Fine-Tuning JSONL format and save.\"\"\"\n",
    "    with open(filename, \"w\") as jsonl_file:\n",
    "        for _, row in dataframe.iterrows():\n",
    "            user_prompt = (\n",
    "                f\"Analyze the following {row['language']} code for security flaws.\\n\\n\"\n",
    "                f\"**Vulnerability Type:** {row['vulnerability']}\\n\"\n",
    "                f\"**System Affected:** {row['system']}\\n\"\n",
    "                f\"**Prompt:** {row['question']}\\n\\n\"\n",
    "                f\"**Insecure Code:**\\n```{row['language'].lower()}\\n{row['insecure_code']}\\n```\\n\\n\"\n",
    "                f\"**Secure Code:**\\n```{row['language'].lower()}\\n{row['secure_code']}\\n```\\n\\n\"\n",
    "                f\"Explain the vulnerabilities in the insecure version and why the secure version is better.\"\n",
    "            )\n",
    "\n",
    "            entry = {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a cybersecurity expert specializing in vulnerability detection.\"},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt},\n",
    "                    {\"role\": \"assistant\", \"content\": \"This code has security flaws due to unsafe input handling. A secure implementation uses proper input validation and sanitization.\"}\n",
    "                ]\n",
    "            }\n",
    "            jsonl_file.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "\n",
    "# Save Train & Validation Sets Separately\n",
    "save_to_jsonl(train_df, \"gpt4o_php_python_train.jsonl\")\n",
    "save_to_jsonl(val_df, \"gpt4o_php_python_val.jsonl\")\n",
    "\n",
    "print(f\" Train JSONL created: 'gpt4o_php_python_train.jsonl' ({len(train_df)} samples)\")\n",
    "print(f\" Validation JSONL created: 'gpt4o_php_python_val.jsonl' ({len(val_df)} samples)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train JSONL created: 'gpt4o_cpp_train.jsonl' (350 samples)\n",
      "‚úÖ Validation JSONL created: 'gpt4o_cpp_val.jsonl' (150 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lanze\\AppData\\Local\\Temp\\ipykernel_22740\\70385261.py:6: DtypeWarning: Columns (5,6,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_cpp = pd.read_csv(\"c++.csv\", encoding=\"utf-8\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load C++ Dataset\n",
    "df_cpp = pd.read_csv(\"c++.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# Fill missing values\n",
    "df_cpp = df_cpp.fillna(\"Unknown\")\n",
    "\n",
    "# Rename columns for consistency\n",
    "df_cpp.rename(columns={\"func\": \"code\", \"target\": \"vulnerability\"}, inplace=True)\n",
    "\n",
    "# Limit Dataset to 1,000 Samples\n",
    "df_cpp = df_cpp.sample(n=500, random_state=42)  # Select 500 random samples\n",
    "\n",
    "# Split Data into 70% Train & 30% Validation\n",
    "train_df, val_df = train_test_split(df_cpp, test_size=0.3, random_state=42)\n",
    "\n",
    "# Function to Extract Vulnerable Line(s)\n",
    "def extract_vulnerable_lines(code):\n",
    "    lines = code.split(\"\\n\")\n",
    "    vulnerable_lines = []\n",
    "    for i, line in enumerate(lines):\n",
    "        if any(keyword in line for keyword in [\"strcpy\", \"gets\", \"free\", \"malloc\", \"printf\", \"new\", \"delete\"]):\n",
    "            vulnerable_lines.append(f\"line {i+1}: {line.strip()}\")\n",
    "    return \"\\n\".join(vulnerable_lines) if vulnerable_lines else \"No specific lines detected.\"\n",
    "\n",
    "# Function to Generate Secure Fix\n",
    "def generate_secure_fix(code):\n",
    "    secure_code = code.replace(\"strcpy\", \"strncpy\").replace(\"gets\", \"fgets\")  # Example fixes\n",
    "    return secure_code if secure_code != code else \"Use memory-safe functions and validate inputs.\"\n",
    "\n",
    "# Function to Save Data as JSONL\n",
    "def save_to_jsonl(dataframe, filename):\n",
    "    \"\"\"Convert dataset to OpenAI Fine-Tuning JSONL format and save.\"\"\"\n",
    "    with open(filename, \"w\") as jsonl_file:\n",
    "        for _, row in dataframe.iterrows():\n",
    "            vulnerable_lines = extract_vulnerable_lines(row['code'])\n",
    "            secure_fix = generate_secure_fix(row['code'])\n",
    "\n",
    "            user_prompt = (\n",
    "                f\"Analyze the following C++ code for security flaws.\\n\\n\"\n",
    "                f\"**Vulnerability Type:** {row['vulnerability']}\\n\"\n",
    "                f\"**Project:** {row['project']}\\n\"\n",
    "                f\"**Commit ID:** {row['commit_id']}\\n\"\n",
    "                f\"**Hash:** {row['hash']}\\n\"\n",
    "                f\"**Size:** {row['size']}\\n\"\n",
    "                f\"**Additional Information:** {row['message']}\\n\\n\"\n",
    "                f\"**Code:**\\n```cpp\\n{row['code']}\\n```\\n\\n\"\n",
    "                f\"Explain the vulnerabilities and suggest remediation.\"\n",
    "            )\n",
    "\n",
    "            assistant_response = (\n",
    "                f\"**üîç Vulnerable Line(s):**\\n```\\n{vulnerable_lines}\\n```\\n\\n\"\n",
    "                f\"**üõë Explanation of Vulnerabilities:**\\n\"\n",
    "                f\"- This code contains memory handling issues that could lead to security vulnerabilities such as buffer overflows and use-after-free.\\n\\n\"\n",
    "                f\"**‚úÖ Secure Code Fix:**\\n```cpp\\n{secure_fix}\\n```\\n\\n\"\n",
    "                f\"**üîÑ Explanation of Fix:**\\n- The updated code implements safe memory handling to prevent security risks.\"\n",
    "            )\n",
    "\n",
    "            entry = {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a cybersecurity expert specializing in C++ vulnerability detection.\"},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt},\n",
    "                    {\"role\": \"assistant\", \"content\": assistant_response}\n",
    "                ]\n",
    "            }\n",
    "            jsonl_file.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "\n",
    "# Save Train & Validation Sets Separately\n",
    "save_to_jsonl(train_df, \"gpt4o_cpp_train.jsonl\")\n",
    "save_to_jsonl(val_df, \"gpt4o_cpp_val.jsonl\")\n",
    "\n",
    "print(f\"‚úÖ Train JSONL created: 'gpt4o_cpp_train.jsonl' ({len(train_df)} samples)\")\n",
    "print(f\"‚úÖ Validation JSONL created: 'gpt4o_cpp_val.jsonl' ({len(val_df)} samples)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# python and php\n",
    "job = client.fine_tuning.jobs.create(\n",
    "    # ID after curling file\n",
    "    training_file=\"file-AYYQVSZ569SP1MRAPaHDJQ\",\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "# c++\n",
    "#job = client.fine_tuning.jobs.create(\n",
    "#    # ID after curling file\n",
    "#    training_file=\"file-F77TjyGiKuXXSQrpxxY4dB\",\n",
    "#    model=\"gpt-4o-mini-2024-07-18\",\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PHP & Python Model Accuracy: 100.00%\n",
      "‚úÖ C++ Model Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import json\n",
    "from difflib import SequenceMatcher\n",
    "import numpy as np\n",
    "\n",
    "# Fine-tuned model IDs (Replace with actual model IDs)\n",
    "php_python_model = \"ft:gpt-4o-mini-2024-07-18:websec::B5sm4qG6\"\n",
    "cpp_model = \"ft:gpt-4o-mini-2024-07-18:websec::B5uKtVd6\"\n",
    "\n",
    "# Load validation data\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Load JSONL validation dataset.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return [json.loads(line) for line in file]\n",
    "\n",
    "# Compute similarity score\n",
    "def similarity_score(a, b):\n",
    "    \"\"\"Compute similarity between two text outputs.\"\"\"\n",
    "    return SequenceMatcher(None, a, b).ratio() * 100  # Percentage similarity\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model_id, test_data):\n",
    "    \"\"\"Evaluate fine-tuned GPT-4o model on the test set.\"\"\"\n",
    "    scores = []\n",
    "    for sample in test_data:\n",
    "        user_input = sample[\"messages\"][1][\"content\"]  # Extract user prompt\n",
    "        expected_output = sample[\"messages\"][2][\"content\"]  # Expected assistant response\n",
    "\n",
    "        # Query the fine-tuned model\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_id,\n",
    "            messages=[{\"role\": \"user\", \"content\": user_input}]\n",
    "        )\n",
    "\n",
    "        # Extract model response\n",
    "        model_output = response.choices[0].message.content\n",
    "\n",
    "        # Compute similarity between expected and model output\n",
    "        score = similarity_score(expected_output, model_output)\n",
    "        scores.append(score)\n",
    "\n",
    "    # Compute overall accuracy\n",
    "    avg_accuracy = np.mean(scores)\n",
    "    return avg_accuracy, scores\n",
    "\n",
    "# Load test datasets\n",
    "php_python_test = load_jsonl(\"gpt4o_php_python_val.jsonl\")\n",
    "cpp_test = load_jsonl(\"gpt4o_cpp_val.jsonl\")\n",
    "\n",
    "# Evaluate models\n",
    "php_python_accuracy, php_python_scores = evaluate_model(php_python_model, php_python_test)\n",
    "cpp_accuracy, cpp_scores = evaluate_model(cpp_model, cpp_test)\n",
    "\n",
    "# Display results\n",
    "print(f\" PHP & Python Model Accuracy: {php_python_accuracy:.2f}%\")\n",
    "print(f\" C++ Model Accuracy: {cpp_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'gpt4o_vulnerability_test.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Read JSONL test data\u001b[39;00m\n\u001b[0;32m      5\u001b[0m test_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[0;32m      8\u001b[0m         test_data\u001b[38;5;241m.\u001b[39mappend(json\u001b[38;5;241m.\u001b[39mloads(line))  \u001b[38;5;66;03m# Load each JSONL entry\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'gpt4o_vulnerability_test.jsonl'"
     ]
    }
   ],
   "source": [
    "# Load the test dataset\n",
    "test_file = \"gpt4o_vulnerability_test.jsonl\"\n",
    "\n",
    "# Read JSONL test data\n",
    "test_data = []\n",
    "with open(test_file, \"r\") as file:\n",
    "    for line in file:\n",
    "        test_data.append(json.loads(line))  # Load each JSONL entry\n",
    "\n",
    "# Fine-tuned model ID (Replace with your actual model ID)\n",
    "fine_tuned_model = \"ft:gpt-4o-mini-2024-07-18:websec::B1W99cp2\"  # Update with your model ID\n",
    "\n",
    "# Function to evaluate the test set\n",
    "def evaluate_model(test_samples):\n",
    "    results = []\n",
    "    for sample in test_samples:\n",
    "        user_input = sample[\"messages\"][1][\"content\"]  # Extract prompt\n",
    "\n",
    "        # Run model inference\n",
    "        response = client.chat.completions.create(\n",
    "            model=fine_tuned_model,\n",
    "            messages=[{\"role\": \"user\", \"content\": user_input}]\n",
    "        )\n",
    "\n",
    "        # Save response\n",
    "        model_output = response.choices[0].message.content\n",
    "        results.append({\n",
    "            \"input\": user_input,\n",
    "            \"expected_output\": sample[\"messages\"][2][\"content\"],  # Expected assistant response\n",
    "            \"model_output\": model_output\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_model(test_data[:85])\n",
    "\n",
    "# Save results to JSON\n",
    "with open(\"gpt4o_vulnerability_test_results.json\", \"w\") as result_file:\n",
    "    json.dump(evaluation_results, result_file, indent=4)\n",
    "\n",
    "print(f\"‚úÖ Evaluation complete. Results saved to 'gpt4o_vulnerability_test_results.json'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sending Request/Message to GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_finetuned_model(user_message, code_snippet):\n",
    "    \"\"\"Send the user input along with the provided code snippet to GPT-4o.\"\"\"\n",
    "    full_prompt = f\"\"\"\n",
    "    {user_message}\n",
    "\n",
    "    ```python\n",
    "    {code_snippet}\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=fine_tuned_model,\n",
    "            messages=[{\"role\": \"user\", \"content\": full_prompt}]\n",
    "        )\n",
    "\n",
    "        # Extract and return response from GPT-4o\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    except client.error.OpenAIError as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "user_input = \"can you analyze the code and tell me if it is secure? if its insecure can you provide me the line of code where it is insecure and give me the remediation of it\"\n",
    "code_snippet = \"def unsafe(): return eval(input())\"  # Pass the code directly\n",
    "\n",
    "response = query_finetuned_model(user_input, code_snippet)\n",
    "\n",
    "print(\"\\nüîç GPT-4o Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation and analysis for chat model fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tiktoken # for token counting\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 424\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are a cybersecurity expert specializing in vulnerability detection.'}\n",
      "{'role': 'user', 'content': 'Analyze the following Python code and determine if it has security flaws.\\n\\nVulnerability Type:\\nPython\\'s built-in function `eval()` can lead to arbitrary code execution if used improperly.\\n\\nPrompt:\\nWrite a python code that takes user input as Python code and executes it using the built-in `eval()` function. The executed code should be sanitized to prevent arbitrary code execution.\\n\\nInsecure Code:\\n```python\\nimport os  def evaluate_input(user_input):       return eval(user_input)  def main():     user_input = input(\"Enter some Python code to execute: \")     result = evaluate_input(user_input)     print(\"Result:\", result)  if __name__ == \"__main__\":     main()\\n```\\n\\nSecure Code:\\n```python\\nimport ast  class RestrictedPython(ast.NodeTransformer):     \"\"\"     AST NodeTransformer that restricts the allowed Python features.     \"\"\"     def visit_Call(self, node):           allowed_calls = {\\'len\\', \\'str\\', \\'range\\', \\'list\\', \\'dict\\', \\'tuple\\', \\'set\\', \\'frozenset\\', \\'print\\', \\'type\\', \\'id\\', \\'sorted\\', \\'reversed\\', \\'enumerate\\', \\'abs\\', \\'all\\', \\'any\\', \\'bin\\', \\'bool\\', \\'chr\\', \\'complex\\', \\'divmod\\', \\'float\\', \\'hex\\', \\'int\\', \\'max\\', \\'min\\', \\'next\\', \\'oct\\', \\'pow\\', \\'round\\', \\'slice\\', \\'sorted\\', \\'str\\', \\'sum\\'}         if not (isinstance(node.func, ast.Name) and node.func.id in allowed_calls):             raise ValueError(f\"Disallowed call: {ast.unparse(node)}\")         self.generic_visit(node)  def evaluate_input(user_input):      tree = ast.parse(user_input, mode=\\'exec\\')      RestrictedPython().visit(tree)      code = compile(tree, filename=\\'<input>\\', mode=\\'exec\\')      safe_dict = {         \\'__builtins__\\': {},     }      exec(code, {\"__builtins__\": {}}, safe_dict)          return safe_dict  def main():     user_input = input(\"Enter some Python code to execute: \")     try:         result = evaluate_input(user_input)         print(\"Result:\", result)     except Exception as e:         print(\"Error:\", e)  if __name__ == \"__main__\":     main()\\n```\\n\\nExplain the vulnerabilities in the insecure version and why the secure version is better.'}\n",
      "{'role': 'assistant', 'content': 'The insecure version allows security vulnerabilities due to improper handling of user input. The secure version mitigates these risks by implementing safe coding practices.'}\n"
     ]
    }
   ],
   "source": [
    "data_path = \"gpt4o_vulnerability_finetune.jsonl\"\n",
    "\n",
    "# Load the dataset\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 186, 706\n",
      "mean / median: 361.9811320754717, 350.0\n",
      "p5 / p95: 272.3, 467.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 27, 27\n",
      "mean / median: 27.0, 27.0\n",
      "p5 / p95: 27.0, 27.0\n",
      "\n",
      "0 examples may be over the 16,385 token limit, they will be truncated during fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 16385 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 16,385 token limit, they will be truncated during fine-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~153480 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~460440 tokens\n"
     ]
    }
   ],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 16385\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
