{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     lang                                      vulnerability  system  \\\n",
      "0  python  Python's built-in function `eval()` can lead t...     NaN   \n",
      "1  python  The use of 'eval' function in Python can lead ...     NaN   \n",
      "2  python  Python's eval() function can lead to arbitrary...     NaN   \n",
      "3  python  The Python `eval()` function can lead to arbit...     NaN   \n",
      "4  python  A buffer overflow vulnerability in Python coul...     NaN   \n",
      "\n",
      "                                            question  \\\n",
      "0  Write a python code that takes user input as P...   \n",
      "1  Write a python code that creates a web server ...   \n",
      "2  Write a python code that imports the os module...   \n",
      "3  Write a python code that defines a function na...   \n",
      "4  Write a python code that imports the os module...   \n",
      "\n",
      "                                              chosen  \\\n",
      "0  import ast\\n\\nclass RestrictedPython(ast.NodeT...   \n",
      "1  from flask import Flask, request, jsonify\\nimp...   \n",
      "2  import subprocess\\n\\ndef run_command(user_inpu...   \n",
      "3  def safe_function():\\n    user_input = input(\"...   \n",
      "4  import subprocess\\n\\ndef run_command(user_inpu...   \n",
      "\n",
      "                                            rejected  \n",
      "0  import os\\n\\ndef evaluate_input(user_input):\\n...  \n",
      "1  import flask\\napp = flask.Flask(__name__)\\n\\n@...  \n",
      "2  import os\\n\\ndef run_command(user_input):\\n   ...  \n",
      "3  def unsafe_function():\\n    user_input = input...  \n",
      "4  import os\\ndef run_command(user_input):\\n    c...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"python_vuln_CyberNative.csv\", encoding=\"utf-8\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data saved as 'gpt4o_vulnerability_train.jsonl' with 339 samples.\n",
      "Test data saved as 'gpt4o_vulnerability_test.jsonl' with 85 samples.\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where both chosen and rejected are NaN\n",
    "df = df.dropna(subset=['chosen', 'rejected'], how='all')\n",
    "\n",
    "# Fill missing values with placeholders\n",
    "df['chosen'] = df['chosen'].fillna(\"No secure version available.\")\n",
    "df['rejected'] = df['rejected'].fillna(\"No insecure version available.\")\n",
    "df['vulnerability'] = df['vulnerability'].fillna(\"Unknown vulnerability\")\n",
    "df['question'] = df['question'].fillna(\"No specific prompt provided.\")\n",
    "\n",
    "# Convert all columns to string format to avoid NaN issues\n",
    "df = df.astype(str)\n",
    "\n",
    "# Normalize newlines and whitespace\n",
    "df['chosen'] = df['chosen'].str.replace(\"\\n\", \" \", regex=True).str.strip()\n",
    "df['rejected'] = df['rejected'].str.replace(\"\\n\", \" \", regex=True).str.strip()\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "def save_to_jsonl(dataframe, filename):\n",
    "    \"\"\"Convert dataset to OpenAI Fine-Tuning JSONL format and save.\"\"\"\n",
    "    with open(filename, \"w\") as jsonl_file:  # 'w' ensures a new file is created\n",
    "        for _, row in dataframe.iterrows():\n",
    "            entry = {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a cybersecurity expert specializing in vulnerability detection.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Analyze the following Python code and determine if it has security flaws.\\n\\n\"\n",
    "                                                f\"Vulnerability Type:\\n{row['vulnerability']}\\n\\n\"\n",
    "                                                f\"Prompt:\\n{row['question']}\\n\\n\"\n",
    "                                                f\"Insecure Code:\\n```python\\n{row['rejected']}\\n```\\n\\n\"\n",
    "                                                f\"Secure Code:\\n```python\\n{row['chosen']}\\n```\\n\\n\"\n",
    "                                                \"Explain the vulnerabilities in the insecure version and why the secure version is better.\"},\n",
    "                    {\"role\": \"assistant\", \"content\": \"The insecure version allows security vulnerabilities due to improper handling of user input. The secure version mitigates these risks by implementing safe coding practices.\"}\n",
    "                ]\n",
    "            }\n",
    "            jsonl_file.write(json.dumps(entry) + \"\\n\")  # Writes each entry as a separate JSONL line\n",
    "\n",
    "save_to_jsonl(train_df, \"gpt4o_vulnerability_train.jsonl\")\n",
    "save_to_jsonl(test_df, \"gpt4o_vulnerability_test.jsonl\")\n",
    "\n",
    "print(f\"Train data saved as 'gpt4o_vulnerability_train.jsonl' with {len(train_df)} samples.\")\n",
    "print(f\"Test data saved as 'gpt4o_vulnerability_test.jsonl' with {len(test_df)} samples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading API key (should be set in your system variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training AI (don't run this will cost money @.@)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#job = client.fine_tuning.jobs.create(\n",
    "    # ID after curling file\n",
    "#    training_file=\"file-SuFU6dDD8UYBQVgCepZduo\",\n",
    "#    model=\"gpt-4o-mini-2024-07-18\",\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation complete. Results saved to 'gpt4o_vulnerability_test_results.json'\n"
     ]
    }
   ],
   "source": [
    "# Load the test dataset\n",
    "test_file = \"gpt4o_vulnerability_test.jsonl\"\n",
    "\n",
    "# Read JSONL test data\n",
    "test_data = []\n",
    "with open(test_file, \"r\") as file:\n",
    "    for line in file:\n",
    "        test_data.append(json.loads(line))  # Load each JSONL entry\n",
    "\n",
    "# Fine-tuned model ID (Replace with your actual model ID)\n",
    "fine_tuned_model = \"ft:gpt-4o-mini-2024-07-18:websec::B1W99cp2\"  # Update with your model ID\n",
    "\n",
    "# Function to evaluate the test set\n",
    "def evaluate_model(test_samples):\n",
    "    results = []\n",
    "    for sample in test_samples:\n",
    "        user_input = sample[\"messages\"][1][\"content\"]  # Extract prompt\n",
    "\n",
    "        # Run model inference\n",
    "        response = client.chat.completions.create(\n",
    "            model=fine_tuned_model,\n",
    "            messages=[{\"role\": \"user\", \"content\": user_input}]\n",
    "        )\n",
    "\n",
    "        # Save response\n",
    "        model_output = response.choices[0].message.content\n",
    "        results.append({\n",
    "            \"input\": user_input,\n",
    "            \"expected_output\": sample[\"messages\"][2][\"content\"],  # Expected assistant response\n",
    "            \"model_output\": model_output\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_model(test_data[:85])\n",
    "\n",
    "# Save results to JSON\n",
    "with open(\"gpt4o_vulnerability_test_results.json\", \"w\") as result_file:\n",
    "    json.dump(evaluation_results, result_file, indent=4)\n",
    "\n",
    "print(f\"✅ Evaluation complete. Results saved to 'gpt4o_vulnerability_test_results.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Load results\n",
    "with open(\"gpt4o_vulnerability_test_results.json\", \"r\") as file:\n",
    "    results = json.load(file)\n",
    "\n",
    "# Compute similarity score\n",
    "def similarity_score(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio() * 100\n",
    "\n",
    "# Evaluate similarity for each test case\n",
    "scores = [similarity_score(item[\"expected_output\"], item[\"model_output\"]) for item in results]\n",
    "\n",
    "# Compute average accuracy\n",
    "average_accuracy = sum(scores) / len(scores)\n",
    "print(f\"✅ Model Accuracy: {average_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation and analysis for chat model fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tiktoken # for token counting\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 424\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are a cybersecurity expert specializing in vulnerability detection.'}\n",
      "{'role': 'user', 'content': 'Analyze the following Python code and determine if it has security flaws.\\n\\nVulnerability Type:\\nPython\\'s built-in function `eval()` can lead to arbitrary code execution if used improperly.\\n\\nPrompt:\\nWrite a python code that takes user input as Python code and executes it using the built-in `eval()` function. The executed code should be sanitized to prevent arbitrary code execution.\\n\\nInsecure Code:\\n```python\\nimport os  def evaluate_input(user_input):       return eval(user_input)  def main():     user_input = input(\"Enter some Python code to execute: \")     result = evaluate_input(user_input)     print(\"Result:\", result)  if __name__ == \"__main__\":     main()\\n```\\n\\nSecure Code:\\n```python\\nimport ast  class RestrictedPython(ast.NodeTransformer):     \"\"\"     AST NodeTransformer that restricts the allowed Python features.     \"\"\"     def visit_Call(self, node):           allowed_calls = {\\'len\\', \\'str\\', \\'range\\', \\'list\\', \\'dict\\', \\'tuple\\', \\'set\\', \\'frozenset\\', \\'print\\', \\'type\\', \\'id\\', \\'sorted\\', \\'reversed\\', \\'enumerate\\', \\'abs\\', \\'all\\', \\'any\\', \\'bin\\', \\'bool\\', \\'chr\\', \\'complex\\', \\'divmod\\', \\'float\\', \\'hex\\', \\'int\\', \\'max\\', \\'min\\', \\'next\\', \\'oct\\', \\'pow\\', \\'round\\', \\'slice\\', \\'sorted\\', \\'str\\', \\'sum\\'}         if not (isinstance(node.func, ast.Name) and node.func.id in allowed_calls):             raise ValueError(f\"Disallowed call: {ast.unparse(node)}\")         self.generic_visit(node)  def evaluate_input(user_input):      tree = ast.parse(user_input, mode=\\'exec\\')      RestrictedPython().visit(tree)      code = compile(tree, filename=\\'<input>\\', mode=\\'exec\\')      safe_dict = {         \\'__builtins__\\': {},     }      exec(code, {\"__builtins__\": {}}, safe_dict)          return safe_dict  def main():     user_input = input(\"Enter some Python code to execute: \")     try:         result = evaluate_input(user_input)         print(\"Result:\", result)     except Exception as e:         print(\"Error:\", e)  if __name__ == \"__main__\":     main()\\n```\\n\\nExplain the vulnerabilities in the insecure version and why the secure version is better.'}\n",
      "{'role': 'assistant', 'content': 'The insecure version allows security vulnerabilities due to improper handling of user input. The secure version mitigates these risks by implementing safe coding practices.'}\n"
     ]
    }
   ],
   "source": [
    "data_path = \"gpt4o_vulnerability_finetune.jsonl\"\n",
    "\n",
    "# Load the dataset\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 186, 706\n",
      "mean / median: 361.9811320754717, 350.0\n",
      "p5 / p95: 272.3, 467.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 27, 27\n",
      "mean / median: 27.0, 27.0\n",
      "p5 / p95: 27.0, 27.0\n",
      "\n",
      "0 examples may be over the 16,385 token limit, they will be truncated during fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 16385 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 16,385 token limit, they will be truncated during fine-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~153480 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~460440 tokens\n"
     ]
    }
   ],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 16385\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
